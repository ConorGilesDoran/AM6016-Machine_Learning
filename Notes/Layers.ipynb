{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23adc4",
   "metadata": {},
   "source": [
    "# Sequential\n",
    "Takes the list of layers as input and bundles them together as a simple network, one layer after another. But one could also start with an empty model and add layers afterwards if they wished.\n",
    "\n",
    "# Flatten\n",
    "Simply reformats the data by converting the 2-D array to a 1-D array, no parameters to learn in this layer.\n",
    "\n",
    "Lecture 3\n",
    "\n",
    "# Dense hidden layer\n",
    "The dense layer is a densely or fully connected layer. This layer comprises 128 nodes/neurons. The dense layer is connected to the output of the previous layer, so that each input (e.g. 28x28 pixels) is now connected to each of the 128 nodes/neurons of the dense layer. Each of these connections (784x128 = 100352 connections) has an associated weight value, and each node has an associated bias (128 biases). Matrix-vector multiplication is performed by this layer, enabling the connection between every input of the layer to every output of the layer. The activation function transforms the input values of the layer nodes and introduces the non-linearity into the model which is necessary for the model to be trained and to learn the relationships between the data. The activation function selected here is 'relu'.\n",
    "\n",
    "Lecture 3\n",
    "\n",
    "# Dense output layer\n",
    "The final layer of the matrix is another dense layer, with 10 nodes representing the 10 possible outputs (digits 0-9). When the model has been successfully trained, this layer will assess whether the input of a '4' image is given by the correct output number label (0-9), i.e. does the image input match the number output. The activation function chosen here can be 'softmax' or 'sigmoid'.\n",
    "\n",
    "Lecture 3\n",
    "\n",
    "# Embedding Layer\n",
    "\n",
    "The embedding layer is often used in text classification models. Instead of using one-hot embedding to represent an integer-encoded word as a very large dimensional vector, we can use this layer to to map a specific word to a vector of a significantly lower dimension, such as 16. Each word in a specific block of text will thus be associated with a 16 dimensional vector (number of words in text sample x 16 = number of total paramaters being mapped). This avoids the use of very large dimensional vectors being used, and thus improves computational efficiency. Moreover, the model will learn these mapped paramteres as it trains, and thus certain words that are similar may be mapped to similar embedding vectors. \n",
    "\n",
    "Dimensions = (batch, length of word sequence (text input), embedding output). \n",
    "\n",
    "https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "\n",
    "\n",
    "# Global Average Pooling\n",
    "\n",
    "A GlobalAveragePooling1D layer is used to average the previous layer output (often an embedding layer) over text seuqence dimension. For example, if the input to this layer (following embedding) was of 256x16 dimensions, then this layer will retain only the one vector of 16 elements. This allows large input text sequences to be represented essentially by one low dimensional vector, allowing the model to process inputs of various initial lengths all as a fixed length vector.\n",
    "\n",
    "Lecture 4\n",
    "\n",
    "https://www.tensorflow.org/text/guide/word_embeddings\n",
    "\n",
    "# Dropout Layer\n",
    "\n",
    "A dropout layer prevents a certain fraction of the neuron inputs from getting to the next layer, at random. It does this by randomly silencing or 'dropping' neurons in the network for each step, thus reducing the complexity of the model. \n",
    "\n",
    "This forces the network to not rely on the previous layer, as any one of the previous neurons could be dropped at random. Overall, this allows the weight to be further distributed across all neuron corrections, rather than one neuron of a layer having a particularly high weight value. The model may no longer be forced to correct potential mistakes of previous layers, making it more robust overall.\n",
    "\n",
    "This may help suppress overfitting as the model is now more likely to look for generic features rather than just remembering input-specific features.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "\n",
    "https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n"
   ]
  },
  {
   "attachments": {
    "image-4.png": {
     "image/png": "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"
    },
    "image-5.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "79745169",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Layers\n",
    "https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "https://stackoverflow.com/questions/43306323/keras-conv2d-and-input-channels\n",
    "\n",
    "With simple densely connected networks, image processing can result in a vast number of trainable paramaters, which in turn can result in slow computation but also the problem of overfitting. Additionally, these basic networks cannot process spatial information, and will struggle to detect class-defining features if their positioning differs from image to image. For example, a model may just be learning that an image of a car is always associated with a feature at the top right of an image, and thus will fail to make an accurate predictions if this feature appears in the bottom right of the next car image. \n",
    "\n",
    "Convolutional neural networks (CNNs) are much more suited to image processing, as they allow spatial information to be processed, meaning the model can recognise, and assign learnable paramaters to, class distinguishing features wherever they are to appear in an image. \n",
    "\n",
    "A CNN is able to process this spatial information via kernel convolution, which involves the use of a kernel/filter of a specified dimension (e.g. 3x3). The filter will pass over the entire image array, moving over the array in portions that match the filter dimensions and the specified stride length. Each time it hovers over one of these portions, it will perform a matrix multiplication between the values within the filter matrix and the values within the specific portion of the image array. The filter moves to the right with each calcualtion until it has completed the width of the image array. It well then move down to the next row of the array and continue with the convolving process, moving over the image to the right once again.\n",
    "\n",
    "In CNNs, the number of filters to be used can be specified, with each filter initially containing randomized values that will be continuously updated throughout the convolution process as the network trains itself. Many filters are used in order to detect the multiple features of an image, and with each convolutional layer that is added, filters will recognise theimportant features most relevant to the predicted classifications.\n",
    "\n",
    "Each time the kernel has been applied over the complete image, a 2D feature map is generated. Once the feature map has been generated for each kernel, this information is then processed by the activation function, which will assess whether a particular image feature is actually present at a certain position in the image. \n",
    "\n",
    "Overall the output from each convolution will be 3D, as each filter is associated with a 2D feature map output.\n",
    "\n",
    "In order to account for image edges during convolution, padding is applied, preventing the 2D feature maps from being downscaled to a smaller size than the original image. Here the padding was specified as 'same' for both layers, meaning the feature maps produced as ouput will be of the same size as the original input image. Another padding option is 'valid', whereby the dimensions of the feature map are reduced in comparison to the original input image,and will end up being of the same dimensions as the filter.\n",
    "\n",
    "(if multiple channels - RGB - check towards data science link)\n",
    "\n",
    "## Conv1D\n",
    "\n",
    "Used for performing kernel convolution along a 1 dimensional input. \n",
    "\n",
    "https://stackoverflow.com/questions/48219121/difference-between-tf-layers-conv1d-vs-tf-layers-conv2d\n",
    "\n",
    "\n",
    "## Conv2D\n",
    "\n",
    "Used for performing kernel convolution along a 2 dimensional input, usually a 2D image.\n",
    "\n",
    "\n",
    "## SeparableConv2D\n",
    "\n",
    "Used for performing a depthwise separable 2D convolution. A depthwise separable convolution involves 2 separate processes, known as depthwise and pointwise convolutions. \n",
    "\n",
    "In a depthwise convolution, each of the RGB colour channels (3) are processed by a separate kernel, with each complete convolution output being stacked together to give an image with 3 channels. \n",
    "\n",
    "\n",
    "Then with the pointwise convolution, a 1x1 kernel is used, therefore moving through every indivdidual point of the input. The kernel will have a depth equal to the numbe of image array channels (3 for RGB). The 1x1x3 kernel is used to process the output image from the depthwise convolution, moving through each individual point across the entirety of the 3 channels. The final output will be a k x k x 1 image. \n",
    "\n",
    "\n",
    "Depending on the number of kernels selected, the complete output will be k x k x 256 (for example).\n",
    "\n",
    "\n",
    "This reduces the number of times the image must be convolved, thus reducing the number of computations, and enhancing computational efficiency. \n",
    "\n",
    "https://keras.io/api/layers/convolution_layers/separable_convolution2d/\n",
    "\n",
    "Pictures and explanation:\n",
    "\n",
    "https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\n",
    "\n",
    "\n",
    "# Pooling\n",
    "\n",
    "A pooling layer is a form of dimensionality reduction used in order to reduce the spatial size of the 3D convolved feature map outputted by the previous convolutional layer. It essentially downsamples or 'pools' the feature map, thus improving computational efficiency of the model.\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "\n",
    "## MaxPooling2D\n",
    "\n",
    "Max Pooling computes the max value from the portion of the image which is in equal in size to the specific kernel dimensions chosen. \n",
    "\n",
    "## AveragePooling2D\n",
    "\n",
    "Average Pooling operates similarly to max, but instead calculates the average value of the specified portion it is processing.\n",
    "\n",
    "More in the comprehensive towards data science link.\n",
    "\n",
    "\n",
    "## GlobalMaxPooling1D and GlobalAveragePooling2D\n",
    "\n",
    "Global means it will calculate the max and average across the entire input, and not just for a specified portion. Only has one output for each set of inputs. \n",
    "\n",
    "https://stats.stackexchange.com/questions/257321/what-is-global-max-pooling-layer-and-what-is-its-advantage-over-maxpooling-layer\n",
    "\n",
    "\n",
    "\n",
    "# BatchNormalization\n",
    "https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338\n",
    "\n",
    "https://deeplizard.com/learn/video/dXB-KQYkzNU\n",
    "\n",
    "https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "\n",
    "Batch normalization is used to normalize the output of a specific layer. It deos this by scaling the activation vectors generated by the activation functions of each node in the layer. \n",
    "\n",
    "First it calculates the mean and variance of the activation vectors, then computes normalization via the z-score formula. The output of this calculation undergoes a linear transfromation as it is mutliplied by an arbitrary parameter, which influences the stdev, the output of which is then added to another parameter, which influences the bias. \n",
    "\n",
    "This is done per batch input established when fitting the model in training.\n",
    "\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "![image-5.png](attachment:image-5.png)\n",
    "\n",
    "\n",
    "The batch normalization has 4 trainable parameters, the mean, variance, gamma (stdev influence), and beta (bias influence).\n",
    "\n",
    "Having normalized outputs accounts for possible network instability tht could arise if a specific weight has a signficantly larger value than other weights in the layer, which would in turn influence the output of neurons downstream of this weighted connection. Increases computational speed of network.\n",
    "\n",
    "\n",
    "\n",
    "## Calculating Paramaters\n",
    "\n",
    "\n",
    "((shape of width of the filter * shape of height of the filter * number of filters in the previous layer+1)*number of filters)\n",
    "\n",
    "last number of filters = biases\n",
    "\n",
    "BATCH NORMLIZATION -> 4 parameters so calc will be 4*number of nodes in previous\n",
    "\n",
    "https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca\n",
    "\n",
    "https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d\n",
    "\n",
    "https://deeplizard.com/learn/video/gmBfb6LNnZs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35ffe1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
