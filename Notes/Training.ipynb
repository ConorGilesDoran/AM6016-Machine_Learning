{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bf2442",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "One hot encoding is used to represent a specific label as a binary vector vector. For example, if the distinct labels for a set of training and testing images were 'dog', 'cat' and 'mouse', each label would be represented as a vector of length 3, with dog = (1., 0., 0.), cat = (0., 1., 0.) and mouse = (0., 0., 1.). Therefore, 'dog' corresponds to index 0, cat corresponds to index 1 and mouse corresponds to index 2. This represents the output the labels as probability distributions, and it allows us to make comparisons with the actual output vector from our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6547b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for one hot encoding\n",
    "import numpy as np\n",
    "animals = np.array(('dog','cat','mouse'))\n",
    "\n",
    "one_hot = []\n",
    "for i in range(len(animals)):\n",
    "    encode = np.zeros((1,len(animals)))\n",
    "    \n",
    "    encode[0,i] = 1\n",
    "    \n",
    "    one_hot.append(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57480777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 0., 0.]]), array([[0., 1., 0.]]), array([[0., 0., 1.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428083e5",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "\n",
    "When training a model, we have a predefined cost/error/loss function that we are aiming to minimise. This loss function determines how well the model is performing in terms of prediction accuracy, usually computed by comparing the output of a network with the desired result. The lower the output of the cost function, the more accurate predictions being made by the model. If the model has made a perfect prediction, the loss function output would be 0. In training, we want to alter the weights of the neuron connections within a network so that the cost function will be minimised effectively. \n",
    "\n",
    "## Categorical Crossentropy\n",
    "\n",
    "n order to assess how the model is performing in its prediction task, we must calculate the loss function output, which will aid the model in adjusting the weights for improving predictiona accuracy. In multi-class classification models, the overall output from our model, when using the softmax actviation function, will be a probability distribution, with each individual probability associated with a specific class (label). Following one hot encoding, the target or desired values we are comparing these probabilities to is also a probability distribution.  \n",
    "\n",
    "Comparing the distance between two distributions is usually calculated using cross-entropy:\n",
    "\n",
    "\\begin{equation}\n",
    "CE = - \\sum_i p_i \\log q_i\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ is the distribution of the label values, and $q_i$ is the distribution of the predicted label values.\n",
    "\n",
    "However, due to one hot encoding, in multi-class classification models, the distribution of the label values is represented as a binary vector, which can be interpreted as a probability distribution, with the index of a specific class being set to 1, while all other class valeus are 0. This allows the cross-entropy to be simplified to the categorical crossentropy calculation, which is just:\n",
    "\n",
    "\\begin{equation}\n",
    "CCE = -\\log q_l\n",
    "\\end{equation}\n",
    "\n",
    "with $q_l$ representing the predicted probabiltiy distribution values for a specific label (class). This makese sense as only the target label in question will be multiplied by 1, while the rest of the label distribution data would be multiplied by 0. \n",
    "\n",
    "## Sparse Categorical Crossentropy\n",
    "\n",
    "Allows labels to be inputted as integers, not requiring a prior one-hot encoding step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a755d40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "softmax_output = np.array([0.7, 0.1, 0.2])\n",
    "\n",
    "label_output = np.array([1,0,0])\n",
    "\n",
    "label_index = np.where(label_output == 1)[0][0]\n",
    "\n",
    "loss = -np.log(softmax_output[label_index])\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067353bd",
   "metadata": {},
   "source": [
    "## Binary Crossentropy\n",
    "\n",
    "Binary crossentropy is often used for binary classification problems, where the actual classes are either labelled 0 or 1. The predicted label outputs will be generated from the sigmoid function of the output layer, and not the softmax as with categorical crossentropy. We can compare the predicted labels to the correct label by using a log loss function, which will penalize inaccurate predictions. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "BCE = \\frac{1}{n} \\sum_{i=1} -(y_i \\log(p_i) + (1-y_i)\\log(1-p_i))\n",
    "\\end{equation}\n",
    "\n",
    "where $y_i$ is the true class label and $p_i$ is the predicted class label value for a specific training/testing sample $i$. As the sigmoid activation function outputs values between 0 and 1, a negative sign is used in this formula in order to convert the log output of these values (which will be negative) into a positive value. To get the the total log loss, all calculations for a particular sample are summed together. Finally, the average log loss across all prediicted samples is computed.\n",
    "\n",
    "\n",
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0408e",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "## Adam\n",
    "\n",
    "Optimizers will learn to modify the weights of the model in order to minimize the loss function. Here the optimzer algorithm is chosen to be an adaptive optimizer known as 'adam', which improves on sctochastic gradient descent and is ideal for sparse data. The training images are an example of sparse data as they contain many zero values. The adam optimizer makes use of random numbers, meaning it can quickly and efficiently minimize the loss funuction. Adam is one of the most succesful optimizers, requiring less memory and being very computationally efficient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72efca32",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Want to keep track of stuff that isnt exactly relevant to model but is important for training diagnostics.\n",
    "\n",
    "accuracy - how likely do i get the right result (different to loss fxn)\n",
    "\n",
    "Take a look at the probability distribution of softmax for example, does the index of this largest value correspond to the correct label? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8347d",
   "metadata": {},
   "source": [
    "#### When compiling the model, it is translated to tensorflow graph, so now the data can be used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361796d2",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "\n",
    "\n",
    "# Epochs\n",
    "\n",
    "The number of epochs corresponds to the number of steps to be used when training the model. Epochs allow the model to train on the entire image dataset numerous times, with each epoch offering the model another attempt at trying to change the weights and improve the minimization of the loss function. \n",
    "\n",
    "Specifying epochs doesnt necessarily mean the model is becomes more accurate, could lead to overfitting. The accuracy may increase as loss decreases with each epoch, but this is not the real accuarcay that is relevant for performance, we must show it unseen data to assess this true prediction accuracy.\n",
    "\n",
    "\n",
    "\n",
    "# Batch Size\n",
    "\n",
    "Feed the model an entire batch of inputs and labels (could be 32 or so all in one go). If, for example, it was a b atch size of 32 and there were 10 possible label outputs, then the model would be asked to predict 32 x 10 outputs in one step. Each of the 10 outputs is a vector of 10 elements.\n",
    "\n",
    "The model will examine each predicted label and calculate each loss function output. It will then sum together all 32 outputs. This is all completed in one step. \n",
    "\n",
    "After each epoch, the weights are changed so that the sum of this batch is minimized. In this way the model isnt just being shown 1 image and trying to change weights, it does this for a batch of 32. This is more computationally efficient, as calculating gradients is an expensive process.\n",
    "\n",
    "Why not bigger batch size? \n",
    "\n",
    "The computed gradients from all inputs would overlap, certain gradients need to increase ad decrease in order to to change for the next calculation. If there are too many, the computer wouldn't know which one to follow (which direction or which weighting is correct).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e44e5",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531b75f",
   "metadata": {},
   "source": [
    " loss fxn calculates how well does model train for this data, if model has correct soln for training data. it just means the training data youve shown is well replicated\n",
    "want it to be able to accurately identify a bag it hasnt seen before as a bag - real purpose of the training\n",
    "models can be overtrained - and rather than understanding what goes on - they just remember what goes on - cant apply it in new situation\n",
    "\n",
    "\n",
    "\n",
    "Overfitting occurs when the model performs excellently with training data but fails to perform well on unseen or testing data. If a model is predicting the correct solution for training data, it just means the training data you have fed into the model is being well replicated, it does not necessarily mean the model will be highly accurate for unseen data. Overfitting needs to be avoided in order to build a reliable neural network that performs well on data it has never seen before (the real purpose of training the model). Overfitting causes a model to memorise, rather than learn from, the data it has been trained upon. The model has not actually learned the general features associated with each label, but instead has just memorised how it achieved a specific output. This results in inaccurate testing predictions with unseen data, as the model cannot generalise the distinguishable features.\n",
    "\n",
    "Overfitting may be detected in training through the use of a validation set, or it may be only detected when actaully evaluating the model on the testing data.\n",
    "\n",
    "One way to overcome overfitting is through the use of more data, which allows the model to learn from more inputs and thus may actually be able to distinguish and generalise class features rather than memorise how an output was geenrated. \n",
    "\n",
    "Another way to prevent overfitting is through data augmentation. This is where the data in our training set is altered in a realistic way, such as flipping, rotating, or zooming images, so that the model cannot memorise associations but actually learns the distinguishable features associated with each target class. This modified data can be used as input in addition to the original, unedited data, creating a larger training dataset. \n",
    "\n",
    "We could also reduce the complexity of layers, such as decreasing the number of nodes.\n",
    "\n",
    "Another technique used to reduce overfitting is the inclusion of a dropout layer.\n",
    "\n",
    "\n",
    "## Dropout Layer\n",
    "\n",
    "A dropout layer prevents a certain fraction of the neuron inputs from getting to the next layer, at random. It does this by randomly silencing or 'dropping' neurons in the network for each step, thus reducing the complexity of the model. \n",
    "\n",
    "This forces the network to not rely on the previous layer, as any one of the previous neurons could be dropped at random. Overall, this allows the weight to be further distributed across all neuron corrections, rather than one neuron of a layer having a particularly high weight value. The model may no longer be forced to correct potential mistakes of previous layers, making it more robust overall.\n",
    "\n",
    "This may help suppress overfitting as the model is now more likely to look for generic features rather than just remembering input-specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6310e9da",
   "metadata": {},
   "source": [
    "#### Underfitting? \n",
    "\n",
    "Cannot classify training data very well at all. \n",
    "\n",
    "Increase complexity of model, choose relevant layers for data.\n",
    "Add more features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046db24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
