{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4b0f7f",
   "metadata": {},
   "source": [
    "# RNN Explanation\n",
    "\n",
    "https://www.youtube.com/watch?v=_h66BW-xNgk\n",
    "\n",
    "Lecture 8\n",
    "\n",
    "\n",
    "Recurrent neural networks (RNNs) are a type of ANN particlularly sutiable for processing sequential information, such as time series data.\n",
    "\n",
    "In a feedforward network, where connections and information always flows in one direction, the input is always a vector of fixed length. \n",
    "\n",
    "In RNNs, the input can be sequences of variable length, meaning paramaters can be shared across the input sequence, long term dependencies can be kept track of, and information relevant to order can be maintained. \n",
    "\n",
    "In an RNN, the network contains a recurrent unit and loop, where internal information is being shared from one time step to the next. This information is used to update and maintain an internal cell state, which can be termed $h_t$, and is calculated by a recurrence relation:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= \\tanh(W_{hh}h_{t-1} + W_{hx} x_t)\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $W$ represents a matrix, with $W_{hh}h_{t-1}$ corresponding to the previous value of $h_t$, and $W_{hx}x_t$ corresponding to a matrix for input. $tanh$ is the non-linear activation function used here.\n",
    "\n",
    "At each time step, the same exact computation is performed. Every time step, we are calculating a new $h$ based on the previous value of $h$. The overall output that is generated at each time step is then given by: \n",
    "\n",
    "\\begin{align}\n",
    "y_t &= W_{yh} h_t\n",
    "\\end{align}\n",
    "\n",
    "As an example, for the first computation, an input at time step 1 ($x_1$) is multiplied by $Whx$, while $Whh$ is multiplied by the initial internal state ($h_0$). Both outputs are summed together and are then taken as input to the $tanh$ function, which generates a specific output for $h$ at this specific time step, $h_1$. This value for $h_1$ is then multiplied by a third matrix $W_{yh}$ to produce the overall output, $y_1$.\n",
    "\n",
    "The value of $h_1$ then gets fed back into the network as it moves to the next input at time step 2 ($x_2$). As before, $h_1$ will be multiplied by $W_{hh}$, and $x_2$ is multiplied by $W_{hx}$, with each output being summed together and processed by the $tanh$ function, ultimately generating $h_2$. Then, $h2$ is multiplied by $W_{yh}$ to generate the second overall output $y_2$.\n",
    "\n",
    "The process will then repeat, with $h_2$ re-entering the network, which now moves to time step 3. This continues for all inputs at each time step. \n",
    "\n",
    "Based on $y$, we can then compute a certain loss function output at each time step ($L_t$), completing a forward pass through the network.\n",
    "\n",
    "The total loss is calculated by summing together all the individual loss values at each time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149ac8a3",
   "metadata": {},
   "source": [
    "# Backpropagation Through Time\n",
    "\n",
    "In feed forward networks, the derivative of the loss function is calculated with respect to each parameter in the network, and this is used to alter the weights within the network in order to minimize the loss function. \n",
    "\n",
    "With RNNs, the cost function depends on all the matrices involved. Therefore, we must calculate the derivative of the three matrices, with respect to any one of these matrices and any matrix element $i,j$. In RNNs, the derivatives are calculated as the algorithm progresses, rather than just at a single time step. Errors are backpropagated at each time step but then also across all time steps, from the current state all the way back to the very initial state. Thus, this method of calcualating derivatives and updating parameters in RNNs is known as 'Backpropagation Through Time'.\n",
    "\n",
    "Notably, $W_{hh}$ has a significant bearing on the output $y_t$, as if you change $W_{hh}$, it will affect $y_1$ and then appear again later to affect $y_2$ and so on. \n",
    "\n",
    "Therefore, $W_{hh}$ influences all output values, and one must be aware of this when calculating derivatives and gradient. \n",
    "\n",
    "This can result in the shrinking and exploding gradient problem. As there are many matrix multiplication operations being performed, if a large number of these values are greater than 1, an 'exploding' gradient problem can be encountered, where the output is so extremely large that no more training can actually be performed. If the opposite occurs and many of the matrix values are less than 1, we can encounter the 'vanishing' gradient problem, where the output is so small that the model becomes untrainable once again, as no more informative data can be gained from training. (Check MIT slides to see how this can be overcome). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d7053",
   "metadata": {},
   "source": [
    " # Long Short Term Memory networks (LSTMs)\n",
    " \n",
    " \n",
    "One of the most robust methods of overcoming this shrink or explode problem is to use more complex systems for keeping track of, and updating, internal cell states. An example of a more complex recurrent unit is a gated cell, with Long Short Term Memory networks (LSTMs) being the gold standard of gated cells for developing RNNs. \n",
    "\n",
    "In a normal RNN, the network has one node for the activation function transformation (the $tanh$ function). THowever, in the recurrent unit of LSTMs, there are actually a number of these computational layers, each involved with interactions between one another that aid in controlling the flow of information within the recurrent network. \n",
    "\n",
    "Instead of the two equations seen with standard RNNs, LSTMs involve 6 equations, adding more complexity to the overall model:\n",
    " \n",
    "\\begin{align}\n",
    "  f_{t}&=\\sigma _{g}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})\\\\\n",
    "  i_{t}&=\\sigma _{g}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})\\\\\n",
    "  o_{t}&=\\sigma _{g}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})\\\\\n",
    "  {\\tilde {c}}_{t}&=\\sigma _{h}(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})\\\\\n",
    "  c_{t}&=f_{t}\\circ c_{t-1}+i_{t}\\circ {\\tilde {c}}_{t}\\\\\n",
    "  h_{t}&=o_{t}\\circ \\sigma _{h}(c_{t})\n",
    " \\end{align}\n",
    " \n",
    " \n",
    "In addition to having $h_t$, the LSTM network also involves a cell state , $c_t$, that it updates and maintains through the use of 'gates', which are network structures involving sigmoid actviation functions and pointwise multiplication processes. These computations essentially regulate the information being used to update the cell state. \n",
    "\n",
    "LSTMS work by:\n",
    "\n",
    "1) Forgetting irrelevant information from the previous state $h_{t-1}$.\n",
    "\n",
    "2) Processing both the previous and current information in order to update the cell state $C_t$.\n",
    "\n",
    "3) Finally returning a cell state that has passed through a specific output gate, undergoing a transformation.\n",
    "\n",
    "Overall, this can be summarised by simply as 'Forget, Update, Output'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
